{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b798629-d99c-45a4-9ee8-24f7babfa57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (4.42.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "   ---------------------------------------- 0.0/547.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 547.8/547.8 kB 17.3 MB/s eta 0:00:00\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "   ---------------------------------------- 0.0/316.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 316.1/316.1 kB 19.1 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl (373 kB)\n",
      "   ---------------------------------------- 0.0/373.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 373.1/373.1 kB 11.7 MB/s eta 0:00:00\n",
      "Downloading pyarrow-16.1.0-cp38-cp38-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.9 MB 25.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 1.5/25.9 MB 16.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.3/25.9 MB 18.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 3.1/25.9 MB 18.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.5/25.9 MB 19.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.6/25.9 MB 18.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.9/25.9 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.3/25.9 MB 21.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.7/25.9 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 11.0/25.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.6/25.9 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.9/25.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.4/25.9 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/25.9 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.9/25.9 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.4/25.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.4/25.9 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.9/25.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.3/25.9 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.8/25.9 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.9/25.9 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "   ---------------------------------------- 0.0/132.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 132.6/132.6 kB ? eta 0:00:00\n",
      "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/10.8 MB 33.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.7/10.8 MB 28.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.3/10.8 MB 34.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.1/10.8 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.8 MB 32.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.0/10.8 MB 32.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/10.8 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 31.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl (29 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, tzdata, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pandas-2.0.3 pyarrow-16.1.0 pyarrow-hotfix-0.6 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f51593-0db0-4db1-aef4-73ef0cf00da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set up Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbed6aaa-925a-4d98-8833-30cd493e592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = cv2.bilateralFilter(image, 5, 55, 60)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, image = cv2.threshold(image, 240, 255, cv2.THRESH_BINARY)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd13b5b-f9d8-425e-9778-55185b0f7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = preprocess_image(image)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    allowed_chars = string.ascii_letters + string.digits + \" \"\n",
    "    filtered_text = \"\".join(char if char in allowed_chars else \" \" for char in text).replace(\"\\n\", \" \")\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e47c83-bc85-4315-a4b9-04831ca8d70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ff0d8286594db59923a22b2c05b66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rohit\\.cache\\huggingface\\hub\\models--Hate-speech-CNERG--dehatebert-mono-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18587f27c9df45d781851f39e3bf59f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f78881843c4ffaac7242a7f86dca5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacc46f5070d41ee8e0eccc001b5b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f9daced927490e8155ee6cffd42100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a pipeline for hate speech detection\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2294766-98bd-4a6a-acca-e917962b562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hate_speech(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    hate_confidence = predictions[0][1].item()  # Assuming the second index corresponds to \"hateful\"\n",
    "    classification = \"hateful\" if hate_confidence > 0.5 else \"not hateful\"\n",
    "    confidence = hate_confidence if classification == \"hateful\" else 1 - hate_confidence\n",
    "    return classification, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46b0263-aae8-440d-bdba-0dffb80ec6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae58264195046ab9752e3c7ad115193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rohit\\.cache\\huggingface\\hub\\models--nlpconnect--vit-gpt2-image-captioning. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ef79ca372f43119cc29cf52d6609f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1454c9c1574a9597a9b2a80928f9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e862ec5f6d4ee78f77d381896e38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244bd50edf8541259c472dafb373fa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a476c17ddc744d08d9ffa0c2636d37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784160280d524e91a36091b9da6bb9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34728759ac3d45adbf44d4f47041f32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained image captioning model and tokenizer\n",
    "caption_model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "caption_model = VisionEncoderDecoderModel.from_pretrained(caption_model_name)\n",
    "caption_feature_extractor = ViTFeatureExtractor.from_pretrained(caption_model_name)\n",
    "caption_tokenizer = AutoTokenizer.from_pretrained(caption_model_name)\n",
    "\n",
    "def generate_image_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = caption_feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    output_ids = caption_model.generate(pixel_values)\n",
    "    caption = caption_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d54ec2-4250-441c-92b1-46bb91b04358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_meme_for_toxicity(image_path, output_file):\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    image_caption = generate_image_caption(image_path)\n",
    "    combined_text = extracted_text + \" \" + image_caption\n",
    "    \n",
    "    # Analyze individually and combined\n",
    "    results = {}\n",
    "    results[\"extracted_text\"] = detect_hate_speech(extracted_text)\n",
    "    results[\"image_caption\"] = detect_hate_speech(image_caption)\n",
    "    results[\"combined_text\"] = detect_hate_speech(combined_text)\n",
    "    \n",
    "    # Write results to file\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(f\"Image File Name: {os.path.basename(image_path)}\\n\")\n",
    "        for text_type, (classification, confidence) in results.items():\n",
    "            file.write(f\"{text_type.capitalize()} - Classification: {classification}, Confidence: {confidence:.4f}\\n\")\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95238d76-44ec-4f47-bebb-cced14ade47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_folder_for_toxicity(folder_path):\n",
    "    output_file = \"detection_results.txt\"\n",
    "    # Ensure the output file is empty before starting\n",
    "    open(output_file, 'w').close()\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            analyze_meme_for_toxicity(image_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef9a522-d702-459d-8edf-8f8b9c01f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "C:\\Users\\rohit\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    }
   ],
   "source": [
    "# Example folder path\n",
    "folder_path = r'C:\\Users\\rohit\\Desktop\\hate\\hateful_memes\\img\\validation\\hateful'\n",
    "\n",
    "# Analyze the folder for toxicity\n",
    "analyze_folder_for_toxicity(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ad9c9-19ce-4297-8841-4b231e023775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
